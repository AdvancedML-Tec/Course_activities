{
 "cells": [
  {
   "attachments": {
    "0d303da7-6904-4aff-a429-726f84f2580a.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAADPCAYAAABfn/UYAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\nDg4ODtMGnvf/AcRfxR3dKXpTAAAAAElFTkSuQmCC\n"
    }
   },
   "cell_type": "markdown",
   "id": "a827d977-8453-43c5-bc54-067480c97dd0",
   "metadata": {},
   "source": [
    "![LogoTec.png](attachment:0d303da7-6904-4aff-a429-726f84f2580a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e89c8",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "### Text Generation\n",
    "#### Team Members:\n",
    "- A01200230 - Armando Bringas Corpus\n",
    "\n",
    "<br>\n",
    "\n",
    "## Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
    "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
    "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Instructions:\n",
    "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
    "\n",
    "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
    "\n",
    "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation. \n",
    "\n",
    "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
    "\n",
    "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
    "\n",
    "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Evaluation Criteria:\n",
    "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
    "\n",
    "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
    "\n",
    "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function. \n",
    "\n",
    "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471fe96-28d5-4d23-b2c1-1ef203c2b750",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb4b117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#PyTorch libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "# Dataloader library\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "# Libraries to prepare the data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# neural layers\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "# Added libraries\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e09c2a9-3780-43fd-8328-c148aece9476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check torch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8ff971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e573a20b-8558-4e43-b57b-55c3e94c6ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1650\n",
      "(7, 5)\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1650', major=7, minor=5, total_memory=4095MB, multi_processor_count=14)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.get_device_capability(0))\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d439e59-a71f-46fd-a2cd-bdf4e8ccaa8b",
   "metadata": {},
   "source": [
    "### Get the train and the test datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6764ae41-178c-4ac9-a338-c65627350859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = WikiText2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4c7dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokeniser = get_tokenizer('basic_english')\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokeniser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c2cb068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "#set unknown token at position 0\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134b832b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "def data_process(raw_text_iter, seq_length = 50):\n",
    "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
    "#     target_data = torch.cat(d)\n",
    "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length), \n",
    "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))  \n",
    "\n",
    "# # Create tensors for the training set\n",
    "x_train, y_train = data_process(train_dataset, seq_length)\n",
    "x_val, y_val = data_process(val_dataset, seq_length)\n",
    "x_test, y_test = data_process(test_dataset, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b54c04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d400fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # choose a batch size that fits your computation resources\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9c5af-99cb-4196-b1a3-5f7da9583139",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LSTM Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898b4c4-2566-484c-9023-46ed0cf3014a",
   "metadata": {},
   "source": [
    "#### LSTM class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff236c1-a3ff-4e9a-b7fd-7466d7f86990",
   "metadata": {},
   "source": [
    "This LSTM-based model is an architecture for handle various sequence processing tasks in NLP. It has a combination of embedding layers, LSTM cells, dropout, and fully connected layers, along with weight initialization. This LSTM-based model is suitable for text generation tasks.\n",
    "\n",
    "In this case we added some initialization for the weights, we check through the PyTorch documentation for the type on initializators: https://pytorch.org/docs/stable/nn.init.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c63b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Feel free to experiment\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # As regularization added dropout in LSTM and after as a layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights() # Added weight layer\n",
    "\n",
    "    def forward(self, text, hidden=None):\n",
    "        embeddings = self.embeddings(text)\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        # As regularization added droput in the output\n",
    "        output = self.dropout(output)\n",
    "        decoded = self.fc(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "    # Added initiallization weights\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # Initialize embeddings\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialize the linear layer\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialization schemes for LSTM: https://pytorch.org/docs/stable/nn.init.html\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                # Glorot - input-hidden weights \n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                # Orthogonal - hidden-hidden weights\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                # Bias \n",
    "                param.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991876f6-d6b9-4616-b767-c69478b3d853",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d924ac65-01e9-4733-a111-1fd2f2af7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimiser):\n",
    "    '''\n",
    "    The following are possible instructions you may want to conside for this function.\n",
    "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
    "    as long as you train your model correctly.\n",
    "        - loop through specified epochs [ok]\n",
    "        - loop through dataloader [ok]\n",
    "        - don't forget to zero grad! [ok]\n",
    "        - place data (both input and target) in device [ok]\n",
    "        - init hidden states e.g. hidden = model.init_hidden(batch_size) [ok]\n",
    "        - run the model [ok]\n",
    "        - compute the cost or loss [ok]\n",
    "        - backpropagation [ok]\n",
    "        - Update paratemers [ok]\n",
    "        - Include print all the information you consider helpful\n",
    "    \n",
    "    '''\n",
    "    # Assign model to current processing device\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    # Start counting total training time\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # Iterate through epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Start epoch timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Put model on training mode\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i, (data, targets) in enumerate(train_loader):\n",
    "            # Assign data to the correct device\n",
    "            data = data.to(device=device, dtype=torch.int64)\n",
    "            targets = targets.to(device=device, dtype=torch.int64)\n",
    "            \n",
    "            # Initialize hidden states\n",
    "            batch_size = data.size(0)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            \n",
    "            # Calculate prediction scores, forward pass\n",
    "            scores, hidden = model(data, hidden)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            scores = scores.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Perform backward pass and optimize\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            # Print batch loss\n",
    "            if i % 100 == 0:  # print every 100 batches\n",
    "                print(f'Epoch {epoch}, Batch {i}, Loss {loss.item()}')\n",
    "        \n",
    "        # Calculate average loss and elapsed time for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Print epoch loss and time\n",
    "        print('\\n' + '-' * 60)\n",
    "        print(f'Epoch {epoch} completed with average loss {avg_loss:.4f} in {elapsed_time:.2f}s')\n",
    "        print('-' * 60 + '\\n')\n",
    "    \n",
    "    # Print total elapsed time for training\n",
    "    total_elapsed_time = time.time() - total_start_time\n",
    "    hours = total_elapsed_time // 3600\n",
    "    minutes = (total_elapsed_time % 3600) // 60\n",
    "    seconds = (total_elapsed_time % 3600) % 60\n",
    "    \n",
    "    # Print total time with hours, minutes and seconds\n",
    "    print(f'Total training time: {int(hours)}h {int(minutes)}m {int(seconds)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7c2d1-cb98-4616-af28-d6b8cfa7c505",
   "metadata": {},
   "source": [
    "### Training LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c9be3-2ec2-4b86-bf50-f07d260bb53b",
   "metadata": {},
   "source": [
    "#### Hyperparameters definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d618261-6f1c-43f9-8964-66fa7a664d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) # vocabulary size\n",
    "emb_size = 800 # embedding size\n",
    "neurons = 512 # the dimension of the feedforward network model, i.e. # of neurons \n",
    "num_layers = 2 # the number of nn.LSTM layers\n",
    "dropout_rate = 0.65 # the regularization factor\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss() # loss function\n",
    "lr = 0.001 # learning rate\n",
    "epochs = 50 # epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a84a28-ab29-4070-8748-49a80543dd29",
   "metadata": {},
   "source": [
    "#### Optimizer, Model definition & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa9c84ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss 10.267952919006348\n",
      "Epoch 0, Batch 100, Loss 6.834283828735352\n",
      "Epoch 0, Batch 200, Loss 6.512574672698975\n",
      "Epoch 0, Batch 300, Loss 6.296056747436523\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 0 completed with average loss 6.7500 in 838.35s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 1, Batch 0, Loss 6.237114906311035\n",
      "Epoch 1, Batch 100, Loss 6.113154411315918\n",
      "Epoch 1, Batch 200, Loss 5.999763011932373\n",
      "Epoch 1, Batch 300, Loss 5.8418450355529785\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 1 completed with average loss 6.0200 in 842.76s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 2, Batch 0, Loss 5.816137790679932\n",
      "Epoch 2, Batch 100, Loss 5.7320122718811035\n",
      "Epoch 2, Batch 200, Loss 5.7492876052856445\n",
      "Epoch 2, Batch 300, Loss 5.564638137817383\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 2 completed with average loss 5.7374 in 862.74s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 3, Batch 0, Loss 5.581632137298584\n",
      "Epoch 3, Batch 100, Loss 5.557146072387695\n",
      "Epoch 3, Batch 200, Loss 5.477694511413574\n",
      "Epoch 3, Batch 300, Loss 5.489266395568848\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 3 completed with average loss 5.5353 in 820.89s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 4, Batch 0, Loss 5.338476657867432\n",
      "Epoch 4, Batch 100, Loss 5.383350372314453\n",
      "Epoch 4, Batch 200, Loss 5.384471416473389\n",
      "Epoch 4, Batch 300, Loss 5.3710174560546875\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 4 completed with average loss 5.3741 in 806.76s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 5, Batch 0, Loss 5.262752056121826\n",
      "Epoch 5, Batch 100, Loss 5.25981330871582\n",
      "Epoch 5, Batch 200, Loss 5.179980278015137\n",
      "Epoch 5, Batch 300, Loss 5.1806793212890625\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 5 completed with average loss 5.2423 in 802.60s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 6, Batch 0, Loss 5.161752700805664\n",
      "Epoch 6, Batch 100, Loss 5.0628252029418945\n",
      "Epoch 6, Batch 200, Loss 5.113908290863037\n",
      "Epoch 6, Batch 300, Loss 5.107614040374756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 6 completed with average loss 5.1287 in 801.80s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 7, Batch 0, Loss 4.99995231628418\n",
      "Epoch 7, Batch 100, Loss 5.022647380828857\n",
      "Epoch 7, Batch 200, Loss 4.963942050933838\n",
      "Epoch 7, Batch 300, Loss 5.053882122039795\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 7 completed with average loss 5.0306 in 802.12s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 8, Batch 0, Loss 4.9972405433654785\n",
      "Epoch 8, Batch 100, Loss 4.8533101081848145\n",
      "Epoch 8, Batch 200, Loss 5.002138614654541\n",
      "Epoch 8, Batch 300, Loss 4.90820837020874\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 8 completed with average loss 4.9443 in 806.23s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 9, Batch 0, Loss 4.737703800201416\n",
      "Epoch 9, Batch 100, Loss 4.832128524780273\n",
      "Epoch 9, Batch 200, Loss 4.884164333343506\n",
      "Epoch 9, Batch 300, Loss 4.805817127227783\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 9 completed with average loss 4.8652 in 801.65s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 10, Batch 0, Loss 4.735334873199463\n",
      "Epoch 10, Batch 100, Loss 4.825525283813477\n",
      "Epoch 10, Batch 200, Loss 4.815614223480225\n",
      "Epoch 10, Batch 300, Loss 4.752321243286133\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 10 completed with average loss 4.7948 in 801.77s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 11, Batch 0, Loss 4.685385227203369\n",
      "Epoch 11, Batch 100, Loss 4.592047214508057\n",
      "Epoch 11, Batch 200, Loss 4.7573018074035645\n",
      "Epoch 11, Batch 300, Loss 4.764608383178711\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 11 completed with average loss 4.7268 in 802.43s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 12, Batch 0, Loss 4.690743923187256\n",
      "Epoch 12, Batch 100, Loss 4.6815996170043945\n",
      "Epoch 12, Batch 200, Loss 4.666337490081787\n",
      "Epoch 12, Batch 300, Loss 4.731950283050537\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 12 completed with average loss 4.6649 in 801.68s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 13, Batch 0, Loss 4.582937717437744\n",
      "Epoch 13, Batch 100, Loss 4.565176010131836\n",
      "Epoch 13, Batch 200, Loss 4.633476734161377\n",
      "Epoch 13, Batch 300, Loss 4.648616790771484\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 13 completed with average loss 4.6074 in 801.88s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 14, Batch 0, Loss 4.569495677947998\n",
      "Epoch 14, Batch 100, Loss 4.514218807220459\n",
      "Epoch 14, Batch 200, Loss 4.593440532684326\n",
      "Epoch 14, Batch 300, Loss 4.619258880615234\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 14 completed with average loss 4.5501 in 801.76s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 15, Batch 0, Loss 4.426522731781006\n",
      "Epoch 15, Batch 100, Loss 4.542745113372803\n",
      "Epoch 15, Batch 200, Loss 4.467591762542725\n",
      "Epoch 15, Batch 300, Loss 4.468790054321289\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 15 completed with average loss 4.4978 in 801.54s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 16, Batch 0, Loss 4.39603853225708\n",
      "Epoch 16, Batch 100, Loss 4.459595203399658\n",
      "Epoch 16, Batch 200, Loss 4.434098720550537\n",
      "Epoch 16, Batch 300, Loss 4.41222620010376\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 16 completed with average loss 4.4467 in 803.02s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 17, Batch 0, Loss 4.277978420257568\n",
      "Epoch 17, Batch 100, Loss 4.381706714630127\n",
      "Epoch 17, Batch 200, Loss 4.446342945098877\n",
      "Epoch 17, Batch 300, Loss 4.519172191619873\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 17 completed with average loss 4.3969 in 801.29s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 18, Batch 0, Loss 4.328232765197754\n",
      "Epoch 18, Batch 100, Loss 4.3408732414245605\n",
      "Epoch 18, Batch 200, Loss 4.308963298797607\n",
      "Epoch 18, Batch 300, Loss 4.388861656188965\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 18 completed with average loss 4.3506 in 802.73s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 19, Batch 0, Loss 4.2198686599731445\n",
      "Epoch 19, Batch 100, Loss 4.278253078460693\n",
      "Epoch 19, Batch 200, Loss 4.325565338134766\n",
      "Epoch 19, Batch 300, Loss 4.434560298919678\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 19 completed with average loss 4.3052 in 801.49s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 20, Batch 0, Loss 4.153954029083252\n",
      "Epoch 20, Batch 100, Loss 4.1899333000183105\n",
      "Epoch 20, Batch 200, Loss 4.23619270324707\n",
      "Epoch 20, Batch 300, Loss 4.235925674438477\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 20 completed with average loss 4.2602 in 801.93s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 21, Batch 0, Loss 4.25771951675415\n",
      "Epoch 21, Batch 100, Loss 4.229190826416016\n",
      "Epoch 21, Batch 200, Loss 4.166518688201904\n",
      "Epoch 21, Batch 300, Loss 4.2935404777526855\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 21 completed with average loss 4.2173 in 802.73s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 22, Batch 0, Loss 4.101994037628174\n",
      "Epoch 22, Batch 100, Loss 4.192526340484619\n",
      "Epoch 22, Batch 200, Loss 4.2172346115112305\n",
      "Epoch 22, Batch 300, Loss 4.196609973907471\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 22 completed with average loss 4.1762 in 801.80s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 23, Batch 0, Loss 4.022215366363525\n",
      "Epoch 23, Batch 100, Loss 4.1735615730285645\n",
      "Epoch 23, Batch 200, Loss 4.181331157684326\n",
      "Epoch 23, Batch 300, Loss 4.131230354309082\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 23 completed with average loss 4.1342 in 801.96s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 24, Batch 0, Loss 3.9640164375305176\n",
      "Epoch 24, Batch 100, Loss 4.088408946990967\n",
      "Epoch 24, Batch 200, Loss 4.200056076049805\n",
      "Epoch 24, Batch 300, Loss 4.156118392944336\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 24 completed with average loss 4.0960 in 801.26s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 25, Batch 0, Loss 3.9424755573272705\n",
      "Epoch 25, Batch 100, Loss 4.053533554077148\n",
      "Epoch 25, Batch 200, Loss 4.033069133758545\n",
      "Epoch 25, Batch 300, Loss 4.1308674812316895\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 25 completed with average loss 4.0578 in 802.33s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 26, Batch 0, Loss 3.940448522567749\n",
      "Epoch 26, Batch 100, Loss 3.9569079875946045\n",
      "Epoch 26, Batch 200, Loss 3.9917783737182617\n",
      "Epoch 26, Batch 300, Loss 4.041839122772217\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 26 completed with average loss 4.0212 in 802.05s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 27, Batch 0, Loss 4.009219169616699\n",
      "Epoch 27, Batch 100, Loss 4.043494701385498\n",
      "Epoch 27, Batch 200, Loss 3.9369277954101562\n",
      "Epoch 27, Batch 300, Loss 4.0703020095825195\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 27 completed with average loss 3.9848 in 802.79s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 28, Batch 0, Loss 3.8649916648864746\n",
      "Epoch 28, Batch 100, Loss 3.8571088314056396\n",
      "Epoch 28, Batch 200, Loss 3.889878511428833\n",
      "Epoch 28, Batch 300, Loss 3.91973876953125\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 28 completed with average loss 3.9503 in 802.23s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 29, Batch 0, Loss 3.8586084842681885\n",
      "Epoch 29, Batch 100, Loss 3.934353828430176\n",
      "Epoch 29, Batch 200, Loss 4.0065836906433105\n",
      "Epoch 29, Batch 300, Loss 4.006514072418213\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 29 completed with average loss 3.9176 in 802.35s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 30, Batch 0, Loss 3.889730215072632\n",
      "Epoch 30, Batch 100, Loss 3.903712749481201\n",
      "Epoch 30, Batch 200, Loss 3.9084503650665283\n",
      "Epoch 30, Batch 300, Loss 3.945368766784668\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 30 completed with average loss 3.8861 in 802.39s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 31, Batch 0, Loss 3.796024799346924\n",
      "Epoch 31, Batch 100, Loss 3.879132032394409\n",
      "Epoch 31, Batch 200, Loss 3.9028306007385254\n",
      "Epoch 31, Batch 300, Loss 3.911132574081421\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 31 completed with average loss 3.8536 in 801.32s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 32, Batch 0, Loss 3.7424488067626953\n",
      "Epoch 32, Batch 100, Loss 3.8054208755493164\n",
      "Epoch 32, Batch 200, Loss 3.8263790607452393\n",
      "Epoch 32, Batch 300, Loss 3.845512628555298\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 32 completed with average loss 3.8217 in 801.93s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 33, Batch 0, Loss 3.7514727115631104\n",
      "Epoch 33, Batch 100, Loss 3.8061022758483887\n",
      "Epoch 33, Batch 200, Loss 3.736781597137451\n",
      "Epoch 33, Batch 300, Loss 3.8150887489318848\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 33 completed with average loss 3.7936 in 801.93s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 34, Batch 0, Loss 3.6780450344085693\n",
      "Epoch 34, Batch 100, Loss 3.703847646713257\n",
      "Epoch 34, Batch 200, Loss 3.759312629699707\n",
      "Epoch 34, Batch 300, Loss 3.7719740867614746\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 34 completed with average loss 3.7634 in 801.23s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 35, Batch 0, Loss 3.658438205718994\n",
      "Epoch 35, Batch 100, Loss 3.703782081604004\n",
      "Epoch 35, Batch 200, Loss 3.670518159866333\n",
      "Epoch 35, Batch 300, Loss 3.7931385040283203\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 35 completed with average loss 3.7355 in 824.86s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 36, Batch 0, Loss 3.649303913116455\n",
      "Epoch 36, Batch 100, Loss 3.639050006866455\n",
      "Epoch 36, Batch 200, Loss 3.754610538482666\n",
      "Epoch 36, Batch 300, Loss 3.7921905517578125\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 36 completed with average loss 3.7087 in 874.41s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 37, Batch 0, Loss 3.5893630981445312\n",
      "Epoch 37, Batch 100, Loss 3.687694787979126\n",
      "Epoch 37, Batch 200, Loss 3.6619317531585693\n",
      "Epoch 37, Batch 300, Loss 3.6857733726501465\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 37 completed with average loss 3.6824 in 872.09s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 38, Batch 0, Loss 3.641207695007324\n",
      "Epoch 38, Batch 100, Loss 3.6234164237976074\n",
      "Epoch 38, Batch 200, Loss 3.728297233581543\n",
      "Epoch 38, Batch 300, Loss 3.6995999813079834\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 38 completed with average loss 3.6551 in 871.82s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 39, Batch 0, Loss 3.584510087966919\n",
      "Epoch 39, Batch 100, Loss 3.7361221313476562\n",
      "Epoch 39, Batch 200, Loss 3.6356899738311768\n",
      "Epoch 39, Batch 300, Loss 3.6664485931396484\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 39 completed with average loss 3.6325 in 4642.96s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 40, Batch 0, Loss 3.5742011070251465\n",
      "Epoch 40, Batch 100, Loss 3.570418119430542\n",
      "Epoch 40, Batch 200, Loss 3.6597607135772705\n",
      "Epoch 40, Batch 300, Loss 3.684755563735962\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 40 completed with average loss 3.6058 in 863.65s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 41, Batch 0, Loss 3.4757680892944336\n",
      "Epoch 41, Batch 100, Loss 3.449211359024048\n",
      "Epoch 41, Batch 200, Loss 3.6067256927490234\n",
      "Epoch 41, Batch 300, Loss 3.69199275970459\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 41 completed with average loss 3.5829 in 863.91s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 42, Batch 0, Loss 3.4456000328063965\n",
      "Epoch 42, Batch 100, Loss 3.5996456146240234\n",
      "Epoch 42, Batch 200, Loss 3.6827785968780518\n",
      "Epoch 42, Batch 300, Loss 3.5827133655548096\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 42 completed with average loss 3.5613 in 865.51s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 43, Batch 0, Loss 3.4760053157806396\n",
      "Epoch 43, Batch 100, Loss 3.490980625152588\n",
      "Epoch 43, Batch 200, Loss 3.5125222206115723\n",
      "Epoch 43, Batch 300, Loss 3.633470058441162\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 43 completed with average loss 3.5395 in 864.27s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 44, Batch 0, Loss 3.424654245376587\n",
      "Epoch 44, Batch 100, Loss 3.464897394180298\n",
      "Epoch 44, Batch 200, Loss 3.5410289764404297\n",
      "Epoch 44, Batch 300, Loss 3.525073289871216\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 44 completed with average loss 3.5151 in 864.28s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 45, Batch 0, Loss 3.4438858032226562\n",
      "Epoch 45, Batch 100, Loss 3.488055944442749\n",
      "Epoch 45, Batch 200, Loss 3.5350828170776367\n",
      "Epoch 45, Batch 300, Loss 3.5552256107330322\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 45 completed with average loss 3.4941 in 864.46s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 46, Batch 0, Loss 3.396148920059204\n",
      "Epoch 46, Batch 100, Loss 3.5242137908935547\n",
      "Epoch 46, Batch 200, Loss 3.421900987625122\n",
      "Epoch 46, Batch 300, Loss 3.4529242515563965\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 46 completed with average loss 3.4750 in 867.65s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 47, Batch 0, Loss 3.391603469848633\n",
      "Epoch 47, Batch 100, Loss 3.4138152599334717\n",
      "Epoch 47, Batch 200, Loss 3.4773247241973877\n",
      "Epoch 47, Batch 300, Loss 3.473972797393799\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 47 completed with average loss 3.4530 in 863.89s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 48, Batch 0, Loss 3.3335628509521484\n",
      "Epoch 48, Batch 100, Loss 3.4155097007751465\n",
      "Epoch 48, Batch 200, Loss 3.4925928115844727\n",
      "Epoch 48, Batch 300, Loss 3.56992244720459\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 48 completed with average loss 3.4345 in 865.67s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 49, Batch 0, Loss 3.2505855560302734\n",
      "Epoch 49, Batch 100, Loss 3.366615056991577\n",
      "Epoch 49, Batch 200, Loss 3.3907852172851562\n",
      "Epoch 49, Batch 300, Loss 3.4022743701934814\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 49 completed with average loss 3.4144 in 864.08s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Total training time: 12h 29m 29s\n"
     ]
    }
   ],
   "source": [
    "# Call the train function\n",
    "model = LSTMModel(vocab_size, emb_size, neurons, num_layers, dropout_rate)\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, epochs, optimiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10be23-f5f3-4630-bab4-65943bd89c69",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8667411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, num_words, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the starting text and initialize the hidden state\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # Generate words one by one\n",
    "    for _ in range(num_words):\n",
    "        # Input tensor preparation\n",
    "        input_indices = [vocab[word] for word in words[-1:]]  # get indices for the last word only\n",
    "        x = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "        \n",
    "        # Get predictions from model\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        \n",
    "        # Vector of raw prediction score (logit), apply softmax with temperature\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        probabilities = F.softmax(last_word_logits / temperature, dim=0).detach().cpu().numpy()\n",
    "        \n",
    "        # Sample a word index from the probability distribution, append the generated word to the words list\n",
    "        word_index = np.random.choice(len(vocab), p=probabilities)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136187c3-38b4-4d4f-8bc7-6a4c7a4c0f6c",
   "metadata": {},
   "source": [
    "#### Text generations samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53def139-9a2e-426c-b90d-dbc740f145f4",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2884543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like lemon  76 in birmingham . in the 1950s , thomas observed billy the new demon called <unk> and <unk> love america ' s final direction for best story , to celebrate the devin townsend album , when i moved back on dangerously in love in the symphony for fame . the manga ' s album video performances = = = god of war iii film revolution ( father @-@ green , love ) and john ( author william ii [ god ] having a major <unk> ) to be regarded as the seventh to cultural influences . in the\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"I like\", num_words=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a010d3-cd6b-43fa-9457-4ef6695ba0c7",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78eabe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a cat in the midst of a buddhist shadow , with much meat and desired to direct seeing whatever this offensive is in the history of king ' s war . this she was forced to operate in that world . he said that the king was serious on creating my own rivals ( when he made the developing way ) , <unk> it was more careful to this concert rosebery believes to become the person ' s challenging . despite eva pern ' s own tenure , a believe of a criminal family that renewed our imprisonment de <unk> <unk> his things to build within a formula . edmund claims that applewhite and nettles consciously learn , , the humans , said , and substance (\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"A cat\", num_words=125))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ae580-f8a8-4c10-8048-7e7ab13505e1",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb126a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she loves you because the wing ' s pattern were handled . a large tertiary vessel was discovered by group japanese commanders in the united states , where the\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"She loves you because\", num_words=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed5e0d-3bd4-4add-9617-8483185ca4b2",
   "metadata": {},
   "source": [
    "##### Additional examples making some variations with starting text and numer of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6de373d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hope , marking the first time to do there . he and <unk> did not figured on odaenathus ' s name while calvert did not <unk> . <unk> in <unk> , the supreme court of spain did ' s in keeping him as the 22nd century when they began to pursue\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"I hope\", num_words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be96ae3-0322-4e33-a9c9-d4d610b8a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the beatles are able to obtain <unk> as complementary items have been increased . the same list for such <unk> <unk> has not been described as give little social <unk> , this may have been <unk> between the help of a medium and nearest scientific figure . while by inari ii under conservation , latin @-@ americans had a group of different manifestation of it from blood @-@ material if <unk> . after finding more long @-@ derived reactions that specific objects that done in the path , goddesses believed that amun was the most regions of his lives , and the reason of the eighteenth jain @-@ speaking processes . the gods lived in other parts of the history of djedkare and isis , especially in such children , was only an important family of their issues . these objects were found in <unk> , combined with many other piedras phenomena , mostly or visitors to adapted for their myths . the eshmun statue was different , and they emerged in the most stone shiva on tonin . the sculpture begins by osiris , from <unk> , and a south section of an ancient temple held at the manor of maharashtra , a tiger , shows this a long mark to pure a <unk> <unk> . colonel <unk> <unk> a <unk> , which equated with shiva by north @-@ eastern kings with the animal , supported by the wives ' s and personal deities . in magical texts , their only mastery\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"The Beatles are\", num_words=250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c86240a2-b616-41ee-9ed3-89fbbb7f0387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super mario is given the cap of the court around two @-@ thirds of shiva and his left men in this desert . the shape of the facade is given the added walls in the town , which is called <unk> ( the right <unk> ) and and the <unk> @-@ long ( two @-@ square layer ) , at the inscription ( <unk> ) <unk> and the mosque , which contains a plantain meaning being ordered by the capitals . as the mirror is formed around the sculpture of one <unk> , the piers supported the inscription at bath as a small lady holds entrance to the east . each tower has parallel the window for the sky in the caves , which are built over the aisle . fig . 9 is number 66 . <unk> in australia contain surrounded by two short @-@ style organs while simple classic . in addition to each tower ( fig . 16 ) , m @-@ 44 meant the immediately original structure of the <unk> ' building road .\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"Super Mario is\", num_words=175))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6487c8-7554-4b73-9cd0-982916a9e3ed",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855844d-9b8d-40bd-921d-6084fcc15abc",
   "metadata": {},
   "source": [
    "In the initial LSTM class, we incorporated weight initialization to accelerate the model's convergence during training and to potentially enhance overall performance. The following methods were implemented:\n",
    "\n",
    "- Xavier/Glorot initialization for input-hidden weights.\n",
    "- Orthogonal initialization for LSTM hidden-hidden weights.\n",
    "- Zero initialization for biases.\n",
    "\n",
    "When comparing this LSTM, designed for Text Generation, with an RNN used for classification tasks, we observed notable differences. The RNN has a simpler structure focused primarily on prediction, where its performance is evaluated based on accuracy using a cross-entropy loss function. On the other hand, the LSTM, a specialized type of RNN architecture, is more complex. It features memory cells that regulate the flow of information. Unlike the RNN, LSTMs are trained to predict the next token in a sequence based on previous tokens, necessitating the maintenance of information over many time steps. In this case, we computed the cross-entropy loss function. However, for a more accurate evaluation of model performance, it would be appropriate to calculate perplexity.\n",
    "\n",
    "We found this interesting page that very didactically explain the perplexity concept: https://lukesalamone.github.io/posts/perplexity/\n",
    "\n",
    "$$\n",
    "\\text{perplexity} = e^{z}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "z = -\\frac{1}{N} \\sum_{i=0}^{N} \\ln(P_{n})\n",
    "$$\n",
    "\n",
    "Hopefully in the future we can implement an improved model in which we can calculate perplexity. \n",
    "\n",
    "About the temperature, we asked to our dear professor in the class and looks like it is effectively related with Softmax adding a variable theta that affects the softmax distribution, as professor mentioned it can be interpreted of how much entropy or noise you want to have to the output, the more it ism the more \"creative\", we have now the next question, does this parameter could affect the hallucination of the model?\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i \\theta}}{\\sum_{j=0}^{N} e^{z_j \\theta}}\n",
    "$$\n",
    "\n",
    "https://lukesalamone.github.io/posts/what-is-temperature/\n",
    "\n",
    "\n",
    "For the weight initialization and hyperparameters selection we based as well on this Medium article: https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf, for weight initialization they recommend to checked out this paper that has some studies to select different learning rates for regularization and optimization of LSTM models: https://arxiv.org/abs/1708.02182.\n",
    "\n",
    "About the results of the generated text we tried to improve the hyperparameters by increasing the embedding size, neurons and epochs. In the first trial took us like 3 hours to have the model training but in the lasta attempt where we increase the embedding size the computing time creases by four and we didn't noticed relevant chances in the generated texts, we think that is due that this model is quite simple comparing with LLMs like GPT or BERT that incorporate transformers and attention mechanisms to deal with the context. As well, we trained with a very basic and modest setup and a relatively small dataset, the other models are trained with a huge quantity of data that used more demanding computational power. This excersice was very didactic, even we though a little bit dissapointing that we couln't improve over model quality of text generation was a good excersise to start getting familiarized with text generation and LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3264b-19de-45ce-ae6e-fa0df97bf753",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
