{
 "cells": [
  {
   "attachments": {
    "0d303da7-6904-4aff-a429-726f84f2580a.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAADPCAYAAABfn/UYAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\nDg4ODtMGnvf/AcRfxR3dKXpTAAAAAElFTkSuQmCC\n"
    }
   },
   "cell_type": "markdown",
   "id": "a827d977-8453-43c5-bc54-067480c97dd0",
   "metadata": {},
   "source": [
    "![LogoTec.png](attachment:0d303da7-6904-4aff-a429-726f84f2580a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e89c8",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "### Text Generation\n",
    "### Team Members:\n",
    "- A01200230 - Armando Bringas Corpus\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
    "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
    "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Instructions:\n",
    "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
    "\n",
    "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
    "\n",
    "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation. \n",
    "\n",
    "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
    "\n",
    "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
    "\n",
    "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Evaluation Criteria:\n",
    "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
    "\n",
    "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
    "\n",
    "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function. \n",
    "\n",
    "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb4b117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#PyTorch libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "# Dataloader library\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "# Libraries to prepare the data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# neural layers\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "# Added libraries\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e09c2a9-3780-43fd-8328-c148aece9476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check torch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8ff971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e573a20b-8558-4e43-b57b-55c3e94c6ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1650\n",
      "(7, 5)\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1650', major=7, minor=5, total_memory=4095MB, multi_processor_count=14)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.get_device_capability(0))\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6764ae41-178c-4ac9-a338-c65627350859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = WikiText2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4c7dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokeniser = get_tokenizer('basic_english')\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokeniser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c2cb068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "#set unknown token at position 0\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134b832b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "def data_process(raw_text_iter, seq_length = 50):\n",
    "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
    "#     target_data = torch.cat(d)\n",
    "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length), \n",
    "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))  \n",
    "\n",
    "# # Create tensors for the training set\n",
    "x_train, y_train = data_process(train_dataset, seq_length)\n",
    "x_val, y_val = data_process(val_dataset, seq_length)\n",
    "x_test, y_test = data_process(test_dataset, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b54c04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d400fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # choose a batch size that fits your computation resources\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c63b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# Feel free to experiment\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # As regularization added dropout in LSTM and after as a layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights() # Added weight layer\n",
    "\n",
    "    def forward(self, text, hidden=None):\n",
    "        embeddings = self.embeddings(text)\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        # As regularization added droput in the output\n",
    "        output = self.dropout(output)\n",
    "        decoded = self.fc(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "    # Added initiallization weights\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # Initialize embeddings\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialize the linear layer\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialization schemes for LSTM: https://pytorch.org/docs/stable/nn.init.html\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                # Glorot - input-hidden weights \n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                # Orthogonal - hidden-hidden weights\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                # Bias \n",
    "                param.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d618261-6f1c-43f9-8964-66fa7a664d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) # vocabulary size\n",
    "emb_size = 400 # embedding size\n",
    "neurons = 512 # the dimension of the feedforward network model, i.e. # of neurons \n",
    "num_layers = 2 # the number of nn.LSTM layers\n",
    "dropout_rate = 0.65 # the regularization factor\n",
    "\n",
    "model = LSTMModel(vocab_size, emb_size, neurons, num_layers, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d924ac65-01e9-4733-a111-1fd2f2af7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimiser):\n",
    "    '''\n",
    "    The following are possible instructions you may want to conside for this function.\n",
    "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
    "    as long as you train your model correctly.\n",
    "        - loop through specified epochs [ok]\n",
    "        - loop through dataloader [ok]\n",
    "        - don't forget to zero grad! [ok]\n",
    "        - place data (both input and target) in device [ok]\n",
    "        - init hidden states e.g. hidden = model.init_hidden(batch_size) [ok]\n",
    "        - run the model [ok]\n",
    "        - compute the cost or loss [ok]\n",
    "        - backpropagation [ok]\n",
    "        - Update paratemers [ok]\n",
    "        - Include print all the information you consider helpful\n",
    "    \n",
    "    '''\n",
    "    # Assign model to current processing device\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    # Start counting total training time\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # Iterate through epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Start epoch timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Put model on training mode\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i, (data, targets) in enumerate(train_loader):\n",
    "            # Assign data to the correct device\n",
    "            data = data.to(device=device, dtype=torch.int64)\n",
    "            targets = targets.to(device=device, dtype=torch.int64)\n",
    "            \n",
    "            # Initialize hidden states\n",
    "            batch_size = data.size(0)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            \n",
    "            # Calculate prediction scores, forward pass\n",
    "            scores, hidden = model(data, hidden)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            scores = scores.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Perform backward pass and optimize\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            # Print batch loss\n",
    "            if i % 100 == 0:  # print every 100 batches\n",
    "                print(f'Epoch {epoch}, Batch {i}, Loss {loss.item()}')\n",
    "        \n",
    "        # Calculate average loss and elapsed time for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Print epoch loss and time\n",
    "        print('\\n' + '-' * 60)\n",
    "        print(f'Epoch {epoch} completed with average loss {avg_loss:.4f} in {elapsed_time:.2f}s')\n",
    "        print('-' * 60 + '\\n')\n",
    "    \n",
    "    # Print total elapsed time for training\n",
    "    total_elapsed_time = time.time() - total_start_time\n",
    "    hours = total_elapsed_time // 3600\n",
    "    minutes = (total_elapsed_time % 3600) // 60\n",
    "    seconds = (total_elapsed_time % 3600) % 60\n",
    "    \n",
    "    # Print total time with hours, minutes and seconds\n",
    "    print(f'Total training time: {int(hours)}h {int(minutes)}m {int(seconds)}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa9c84ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss 10.26754379272461\n",
      "Epoch 0, Batch 100, Loss 6.810540199279785\n",
      "Epoch 0, Batch 200, Loss 6.568319320678711\n",
      "Epoch 0, Batch 300, Loss 6.375680923461914\n",
      "Epoch 0, Batch 400, Loss 6.357797622680664\n",
      "Epoch 0, Batch 500, Loss 6.107759475708008\n",
      "Epoch 0, Batch 600, Loss 6.2094011306762695\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 0 completed with average loss 6.5176 in 145.80s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 1, Batch 0, Loss 6.070250034332275\n",
      "Epoch 1, Batch 100, Loss 5.9882588386535645\n",
      "Epoch 1, Batch 200, Loss 5.9419660568237305\n",
      "Epoch 1, Batch 300, Loss 5.813576698303223\n",
      "Epoch 1, Batch 400, Loss 5.82686710357666\n",
      "Epoch 1, Batch 500, Loss 5.750176906585693\n",
      "Epoch 1, Batch 600, Loss 5.743387222290039\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 1 completed with average loss 5.8673 in 145.38s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 2, Batch 0, Loss 5.617806911468506\n",
      "Epoch 2, Batch 100, Loss 5.713253021240234\n",
      "Epoch 2, Batch 200, Loss 5.523865222930908\n",
      "Epoch 2, Batch 300, Loss 5.583975791931152\n",
      "Epoch 2, Batch 400, Loss 5.644690036773682\n",
      "Epoch 2, Batch 500, Loss 5.5225300788879395\n",
      "Epoch 2, Batch 600, Loss 5.47500467300415\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 2 completed with average loss 5.6001 in 146.63s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 3, Batch 0, Loss 5.498711109161377\n",
      "Epoch 3, Batch 100, Loss 5.458291053771973\n",
      "Epoch 3, Batch 200, Loss 5.414121627807617\n",
      "Epoch 3, Batch 300, Loss 5.398456573486328\n",
      "Epoch 3, Batch 400, Loss 5.3996429443359375\n",
      "Epoch 3, Batch 500, Loss 5.4339985847473145\n",
      "Epoch 3, Batch 600, Loss 5.480213642120361\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 3 completed with average loss 5.4043 in 144.90s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 4, Batch 0, Loss 5.230963230133057\n",
      "Epoch 4, Batch 100, Loss 5.297327995300293\n",
      "Epoch 4, Batch 200, Loss 5.219820976257324\n",
      "Epoch 4, Batch 300, Loss 5.253423690795898\n",
      "Epoch 4, Batch 400, Loss 5.246250629425049\n",
      "Epoch 4, Batch 500, Loss 5.168619155883789\n",
      "Epoch 4, Batch 600, Loss 5.186792373657227\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 4 completed with average loss 5.2536 in 144.85s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 5, Batch 0, Loss 5.166091442108154\n",
      "Epoch 5, Batch 100, Loss 5.31620979309082\n",
      "Epoch 5, Batch 200, Loss 5.16312313079834\n",
      "Epoch 5, Batch 300, Loss 5.115201950073242\n",
      "Epoch 5, Batch 400, Loss 5.211586952209473\n",
      "Epoch 5, Batch 500, Loss 5.163137435913086\n",
      "Epoch 5, Batch 600, Loss 5.0103535652160645\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 5 completed with average loss 5.1313 in 144.96s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 6, Batch 0, Loss 5.024848461151123\n",
      "Epoch 6, Batch 100, Loss 5.035002708435059\n",
      "Epoch 6, Batch 200, Loss 5.174666881561279\n",
      "Epoch 6, Batch 300, Loss 4.801911354064941\n",
      "Epoch 6, Batch 400, Loss 5.077859878540039\n",
      "Epoch 6, Batch 500, Loss 4.963246822357178\n",
      "Epoch 6, Batch 600, Loss 5.036501407623291\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 6 completed with average loss 5.0274 in 146.92s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 7, Batch 0, Loss 4.891717433929443\n",
      "Epoch 7, Batch 100, Loss 5.018678665161133\n",
      "Epoch 7, Batch 200, Loss 4.986146926879883\n",
      "Epoch 7, Batch 300, Loss 5.0104146003723145\n",
      "Epoch 7, Batch 400, Loss 4.940206050872803\n",
      "Epoch 7, Batch 500, Loss 4.889606475830078\n",
      "Epoch 7, Batch 600, Loss 4.813485145568848\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 7 completed with average loss 4.9382 in 146.11s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 8, Batch 0, Loss 4.921080112457275\n",
      "Epoch 8, Batch 100, Loss 4.758502006530762\n",
      "Epoch 8, Batch 200, Loss 4.9072184562683105\n",
      "Epoch 8, Batch 300, Loss 4.976083755493164\n",
      "Epoch 8, Batch 400, Loss 4.902306079864502\n",
      "Epoch 8, Batch 500, Loss 4.752129554748535\n",
      "Epoch 8, Batch 600, Loss 5.008463382720947\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 8 completed with average loss 4.8608 in 145.90s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 9, Batch 0, Loss 4.634485244750977\n",
      "Epoch 9, Batch 100, Loss 4.820013523101807\n",
      "Epoch 9, Batch 200, Loss 4.7523884773254395\n",
      "Epoch 9, Batch 300, Loss 4.776509761810303\n",
      "Epoch 9, Batch 400, Loss 4.83795690536499\n",
      "Epoch 9, Batch 500, Loss 4.8084306716918945\n",
      "Epoch 9, Batch 600, Loss 4.79347038269043\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 9 completed with average loss 4.7907 in 145.87s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 10, Batch 0, Loss 4.725504398345947\n",
      "Epoch 10, Batch 100, Loss 4.775167465209961\n",
      "Epoch 10, Batch 200, Loss 4.77166223526001\n",
      "Epoch 10, Batch 300, Loss 4.759349822998047\n",
      "Epoch 10, Batch 400, Loss 4.575132369995117\n",
      "Epoch 10, Batch 500, Loss 4.806039810180664\n",
      "Epoch 10, Batch 600, Loss 4.826923847198486\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 10 completed with average loss 4.7267 in 145.85s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 11, Batch 0, Loss 4.531169414520264\n",
      "Epoch 11, Batch 100, Loss 4.604343414306641\n",
      "Epoch 11, Batch 200, Loss 4.761594295501709\n",
      "Epoch 11, Batch 300, Loss 4.7551188468933105\n",
      "Epoch 11, Batch 400, Loss 4.806513786315918\n",
      "Epoch 11, Batch 500, Loss 4.740052700042725\n",
      "Epoch 11, Batch 600, Loss 4.732668399810791\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 11 completed with average loss 4.6681 in 145.82s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 12, Batch 0, Loss 4.554337024688721\n",
      "Epoch 12, Batch 100, Loss 4.61752462387085\n",
      "Epoch 12, Batch 200, Loss 4.458024501800537\n",
      "Epoch 12, Batch 300, Loss 4.673588752746582\n",
      "Epoch 12, Batch 400, Loss 4.701592922210693\n",
      "Epoch 12, Batch 500, Loss 4.638725757598877\n",
      "Epoch 12, Batch 600, Loss 4.672952651977539\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 12 completed with average loss 4.6138 in 145.65s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 13, Batch 0, Loss 4.459351062774658\n",
      "Epoch 13, Batch 100, Loss 4.615110397338867\n",
      "Epoch 13, Batch 200, Loss 4.534666538238525\n",
      "Epoch 13, Batch 300, Loss 4.5922698974609375\n",
      "Epoch 13, Batch 400, Loss 4.577475547790527\n",
      "Epoch 13, Batch 500, Loss 4.651406764984131\n",
      "Epoch 13, Batch 600, Loss 4.627195358276367\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 13 completed with average loss 4.5606 in 145.71s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 14, Batch 0, Loss 4.423804759979248\n",
      "Epoch 14, Batch 100, Loss 4.387869358062744\n",
      "Epoch 14, Batch 200, Loss 4.542606830596924\n",
      "Epoch 14, Batch 300, Loss 4.516372203826904\n",
      "Epoch 14, Batch 400, Loss 4.651638031005859\n",
      "Epoch 14, Batch 500, Loss 4.603548049926758\n",
      "Epoch 14, Batch 600, Loss 4.580609321594238\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 14 completed with average loss 4.5157 in 145.65s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 15, Batch 0, Loss 4.484012603759766\n",
      "Epoch 15, Batch 100, Loss 4.4652323722839355\n",
      "Epoch 15, Batch 200, Loss 4.495882034301758\n",
      "Epoch 15, Batch 300, Loss 4.424421787261963\n",
      "Epoch 15, Batch 400, Loss 4.4547247886657715\n",
      "Epoch 15, Batch 500, Loss 4.436560153961182\n",
      "Epoch 15, Batch 600, Loss 4.524399280548096\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 15 completed with average loss 4.4698 in 145.10s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 16, Batch 0, Loss 4.452652454376221\n",
      "Epoch 16, Batch 100, Loss 4.360368728637695\n",
      "Epoch 16, Batch 200, Loss 4.393150806427002\n",
      "Epoch 16, Batch 300, Loss 4.573128700256348\n",
      "Epoch 16, Batch 400, Loss 4.470964431762695\n",
      "Epoch 16, Batch 500, Loss 4.459651947021484\n",
      "Epoch 16, Batch 600, Loss 4.490174770355225\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 16 completed with average loss 4.4247 in 145.05s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 17, Batch 0, Loss 4.370428085327148\n",
      "Epoch 17, Batch 100, Loss 4.3488545417785645\n",
      "Epoch 17, Batch 200, Loss 4.398737907409668\n",
      "Epoch 17, Batch 300, Loss 4.424504280090332\n",
      "Epoch 17, Batch 400, Loss 4.316903114318848\n",
      "Epoch 17, Batch 500, Loss 4.52170467376709\n",
      "Epoch 17, Batch 600, Loss 4.49298095703125\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 17 completed with average loss 4.3841 in 145.54s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 18, Batch 0, Loss 4.188276290893555\n",
      "Epoch 18, Batch 100, Loss 4.362468719482422\n",
      "Epoch 18, Batch 200, Loss 4.488427639007568\n",
      "Epoch 18, Batch 300, Loss 4.190699100494385\n",
      "Epoch 18, Batch 400, Loss 4.314836025238037\n",
      "Epoch 18, Batch 500, Loss 4.365987777709961\n",
      "Epoch 18, Batch 600, Loss 4.356558322906494\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 18 completed with average loss 4.3442 in 147.88s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 19, Batch 0, Loss 4.306148052215576\n",
      "Epoch 19, Batch 100, Loss 4.3493571281433105\n",
      "Epoch 19, Batch 200, Loss 4.364295482635498\n",
      "Epoch 19, Batch 300, Loss 4.428215980529785\n",
      "Epoch 19, Batch 400, Loss 4.306863784790039\n",
      "Epoch 19, Batch 500, Loss 4.172311782836914\n",
      "Epoch 19, Batch 600, Loss 4.319965362548828\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 19 completed with average loss 4.3061 in 145.87s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 20, Batch 0, Loss 4.276823997497559\n",
      "Epoch 20, Batch 100, Loss 4.150591850280762\n",
      "Epoch 20, Batch 200, Loss 4.3006062507629395\n",
      "Epoch 20, Batch 300, Loss 4.2713775634765625\n",
      "Epoch 20, Batch 400, Loss 4.319421291351318\n",
      "Epoch 20, Batch 500, Loss 4.2852373123168945\n",
      "Epoch 20, Batch 600, Loss 4.272448539733887\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 20 completed with average loss 4.2693 in 145.79s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 21, Batch 0, Loss 4.164339065551758\n",
      "Epoch 21, Batch 100, Loss 4.260614395141602\n",
      "Epoch 21, Batch 200, Loss 4.215837001800537\n",
      "Epoch 21, Batch 300, Loss 4.336123466491699\n",
      "Epoch 21, Batch 400, Loss 4.244368076324463\n",
      "Epoch 21, Batch 500, Loss 4.359288215637207\n",
      "Epoch 21, Batch 600, Loss 4.1466593742370605\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 21 completed with average loss 4.2348 in 145.92s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 22, Batch 0, Loss 4.186697483062744\n",
      "Epoch 22, Batch 100, Loss 4.070662021636963\n",
      "Epoch 22, Batch 200, Loss 4.252056121826172\n",
      "Epoch 22, Batch 300, Loss 4.083205699920654\n",
      "Epoch 22, Batch 400, Loss 4.21652364730835\n",
      "Epoch 22, Batch 500, Loss 4.211496353149414\n",
      "Epoch 22, Batch 600, Loss 4.295970439910889\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 22 completed with average loss 4.2009 in 145.99s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 23, Batch 0, Loss 4.090297698974609\n",
      "Epoch 23, Batch 100, Loss 4.246342182159424\n",
      "Epoch 23, Batch 200, Loss 4.281476974487305\n",
      "Epoch 23, Batch 300, Loss 4.1441545486450195\n",
      "Epoch 23, Batch 400, Loss 4.081283092498779\n",
      "Epoch 23, Batch 500, Loss 4.214148998260498\n",
      "Epoch 23, Batch 600, Loss 4.1916303634643555\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 23 completed with average loss 4.1678 in 146.05s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 24, Batch 0, Loss 4.062864780426025\n",
      "Epoch 24, Batch 100, Loss 4.216592788696289\n",
      "Epoch 24, Batch 200, Loss 4.2941670417785645\n",
      "Epoch 24, Batch 300, Loss 4.1828413009643555\n",
      "Epoch 24, Batch 400, Loss 4.131917953491211\n",
      "Epoch 24, Batch 500, Loss 4.260473251342773\n",
      "Epoch 24, Batch 600, Loss 4.187189102172852\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 24 completed with average loss 4.1359 in 145.69s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 25, Batch 0, Loss 4.090885639190674\n",
      "Epoch 25, Batch 100, Loss 4.080223560333252\n",
      "Epoch 25, Batch 200, Loss 3.9671382904052734\n",
      "Epoch 25, Batch 300, Loss 4.152007579803467\n",
      "Epoch 25, Batch 400, Loss 4.1783766746521\n",
      "Epoch 25, Batch 500, Loss 4.061958312988281\n",
      "Epoch 25, Batch 600, Loss 4.127625942230225\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 25 completed with average loss 4.1072 in 145.14s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 26, Batch 0, Loss 4.145514011383057\n",
      "Epoch 26, Batch 100, Loss 4.087801456451416\n",
      "Epoch 26, Batch 200, Loss 4.074664115905762\n",
      "Epoch 26, Batch 300, Loss 4.084212303161621\n",
      "Epoch 26, Batch 400, Loss 4.056315898895264\n",
      "Epoch 26, Batch 500, Loss 4.130125999450684\n",
      "Epoch 26, Batch 600, Loss 4.08463716506958\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 26 completed with average loss 4.0783 in 145.70s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 27, Batch 0, Loss 4.083858489990234\n",
      "Epoch 27, Batch 100, Loss 3.9826602935791016\n",
      "Epoch 27, Batch 200, Loss 4.177690505981445\n",
      "Epoch 27, Batch 300, Loss 4.048662185668945\n",
      "Epoch 27, Batch 400, Loss 4.043149471282959\n",
      "Epoch 27, Batch 500, Loss 4.07753324508667\n",
      "Epoch 27, Batch 600, Loss 4.185911655426025\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 27 completed with average loss 4.0483 in 146.11s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 28, Batch 0, Loss 4.020688056945801\n",
      "Epoch 28, Batch 100, Loss 3.9358930587768555\n",
      "Epoch 28, Batch 200, Loss 4.008979797363281\n",
      "Epoch 28, Batch 300, Loss 3.991652727127075\n",
      "Epoch 28, Batch 400, Loss 4.064162254333496\n",
      "Epoch 28, Batch 500, Loss 3.9130074977874756\n",
      "Epoch 28, Batch 600, Loss 4.057329177856445\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 28 completed with average loss 4.0196 in 146.01s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 29, Batch 0, Loss 3.9461116790771484\n",
      "Epoch 29, Batch 100, Loss 4.011969089508057\n",
      "Epoch 29, Batch 200, Loss 4.038426399230957\n",
      "Epoch 29, Batch 300, Loss 3.9390182495117188\n",
      "Epoch 29, Batch 400, Loss 3.9837677478790283\n",
      "Epoch 29, Batch 500, Loss 3.9538888931274414\n",
      "Epoch 29, Batch 600, Loss 4.132024765014648\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 29 completed with average loss 3.9929 in 146.01s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 30, Batch 0, Loss 3.899221420288086\n",
      "Epoch 30, Batch 100, Loss 3.9405853748321533\n",
      "Epoch 30, Batch 200, Loss 3.949800729751587\n",
      "Epoch 30, Batch 300, Loss 3.94510555267334\n",
      "Epoch 30, Batch 400, Loss 3.8814504146575928\n",
      "Epoch 30, Batch 500, Loss 4.048482418060303\n",
      "Epoch 30, Batch 600, Loss 4.053915500640869\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 30 completed with average loss 3.9679 in 145.90s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 31, Batch 0, Loss 3.8903005123138428\n",
      "Epoch 31, Batch 100, Loss 3.7792718410491943\n",
      "Epoch 31, Batch 200, Loss 3.9068045616149902\n",
      "Epoch 31, Batch 300, Loss 3.89933443069458\n",
      "Epoch 31, Batch 400, Loss 3.9769856929779053\n",
      "Epoch 31, Batch 500, Loss 3.9355623722076416\n",
      "Epoch 31, Batch 600, Loss 3.98427152633667\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 31 completed with average loss 3.9413 in 145.93s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 32, Batch 0, Loss 3.7422842979431152\n",
      "Epoch 32, Batch 100, Loss 3.969024419784546\n",
      "Epoch 32, Batch 200, Loss 3.838590383529663\n",
      "Epoch 32, Batch 300, Loss 3.930288314819336\n",
      "Epoch 32, Batch 400, Loss 3.9956600666046143\n",
      "Epoch 32, Batch 500, Loss 3.838263750076294\n",
      "Epoch 32, Batch 600, Loss 3.9250378608703613\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 32 completed with average loss 3.9160 in 145.98s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 33, Batch 0, Loss 3.8192436695098877\n",
      "Epoch 33, Batch 100, Loss 3.8198177814483643\n",
      "Epoch 33, Batch 200, Loss 3.8181777000427246\n",
      "Epoch 33, Batch 300, Loss 3.8379604816436768\n",
      "Epoch 33, Batch 400, Loss 3.919612407684326\n",
      "Epoch 33, Batch 500, Loss 3.7745935916900635\n",
      "Epoch 33, Batch 600, Loss 4.0420002937316895\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 33 completed with average loss 3.8939 in 145.96s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 34, Batch 0, Loss 3.8525915145874023\n",
      "Epoch 34, Batch 100, Loss 3.833329200744629\n",
      "Epoch 34, Batch 200, Loss 3.8044216632843018\n",
      "Epoch 34, Batch 300, Loss 3.8663394451141357\n",
      "Epoch 34, Batch 400, Loss 4.01406717300415\n",
      "Epoch 34, Batch 500, Loss 3.8182718753814697\n",
      "Epoch 34, Batch 600, Loss 3.929295539855957\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 34 completed with average loss 3.8704 in 145.93s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 35, Batch 0, Loss 3.7210006713867188\n",
      "Epoch 35, Batch 100, Loss 3.883543014526367\n",
      "Epoch 35, Batch 200, Loss 3.8579463958740234\n",
      "Epoch 35, Batch 300, Loss 3.7534737586975098\n",
      "Epoch 35, Batch 400, Loss 3.825352191925049\n",
      "Epoch 35, Batch 500, Loss 3.9276294708251953\n",
      "Epoch 35, Batch 600, Loss 3.961028814315796\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 35 completed with average loss 3.8484 in 145.90s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 36, Batch 0, Loss 3.8290960788726807\n",
      "Epoch 36, Batch 100, Loss 3.8289148807525635\n",
      "Epoch 36, Batch 200, Loss 3.7234673500061035\n",
      "Epoch 36, Batch 300, Loss 3.8008053302764893\n",
      "Epoch 36, Batch 400, Loss 3.787064552307129\n",
      "Epoch 36, Batch 500, Loss 3.835836410522461\n",
      "Epoch 36, Batch 600, Loss 3.8871941566467285\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 36 completed with average loss 3.8262 in 145.93s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 37, Batch 0, Loss 3.7566187381744385\n",
      "Epoch 37, Batch 100, Loss 3.756960391998291\n",
      "Epoch 37, Batch 200, Loss 3.8185923099517822\n",
      "Epoch 37, Batch 300, Loss 3.8624684810638428\n",
      "Epoch 37, Batch 400, Loss 3.713643789291382\n",
      "Epoch 37, Batch 500, Loss 3.833634614944458\n",
      "Epoch 37, Batch 600, Loss 3.7629222869873047\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 37 completed with average loss 3.8046 in 145.99s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 38, Batch 0, Loss 3.7801647186279297\n",
      "Epoch 38, Batch 100, Loss 3.728842258453369\n",
      "Epoch 38, Batch 200, Loss 3.756453275680542\n",
      "Epoch 38, Batch 300, Loss 3.7213404178619385\n",
      "Epoch 38, Batch 400, Loss 3.883835792541504\n",
      "Epoch 38, Batch 500, Loss 3.843221426010132\n",
      "Epoch 38, Batch 600, Loss 3.848362445831299\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 38 completed with average loss 3.7845 in 146.03s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 39, Batch 0, Loss 3.6050448417663574\n",
      "Epoch 39, Batch 100, Loss 3.7581076622009277\n",
      "Epoch 39, Batch 200, Loss 3.766375780105591\n",
      "Epoch 39, Batch 300, Loss 3.7553343772888184\n",
      "Epoch 39, Batch 400, Loss 3.718255043029785\n",
      "Epoch 39, Batch 500, Loss 3.5381381511688232\n",
      "Epoch 39, Batch 600, Loss 3.814708948135376\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 39 completed with average loss 3.7635 in 146.04s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 40, Batch 0, Loss 3.723541021347046\n",
      "Epoch 40, Batch 100, Loss 3.7785234451293945\n",
      "Epoch 40, Batch 200, Loss 3.7730515003204346\n",
      "Epoch 40, Batch 300, Loss 3.673452377319336\n",
      "Epoch 40, Batch 400, Loss 3.6671698093414307\n",
      "Epoch 40, Batch 500, Loss 3.8380041122436523\n",
      "Epoch 40, Batch 600, Loss 3.8558640480041504\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 40 completed with average loss 3.7442 in 145.70s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 41, Batch 0, Loss 3.634833335876465\n",
      "Epoch 41, Batch 100, Loss 3.631096601486206\n",
      "Epoch 41, Batch 200, Loss 3.6643035411834717\n",
      "Epoch 41, Batch 300, Loss 3.7848446369171143\n",
      "Epoch 41, Batch 400, Loss 3.7590646743774414\n",
      "Epoch 41, Batch 500, Loss 3.723224401473999\n",
      "Epoch 41, Batch 600, Loss 3.9199416637420654\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 41 completed with average loss 3.7254 in 145.44s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 42, Batch 0, Loss 3.716705083847046\n",
      "Epoch 42, Batch 100, Loss 3.6545255184173584\n",
      "Epoch 42, Batch 200, Loss 3.712383985519409\n",
      "Epoch 42, Batch 300, Loss 3.764888286590576\n",
      "Epoch 42, Batch 400, Loss 3.700650215148926\n",
      "Epoch 42, Batch 500, Loss 3.7513327598571777\n",
      "Epoch 42, Batch 600, Loss 3.7737326622009277\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 42 completed with average loss 3.7071 in 145.84s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 43, Batch 0, Loss 3.6419663429260254\n",
      "Epoch 43, Batch 100, Loss 3.67940092086792\n",
      "Epoch 43, Batch 200, Loss 3.6159067153930664\n",
      "Epoch 43, Batch 300, Loss 3.6789331436157227\n",
      "Epoch 43, Batch 400, Loss 3.724299907684326\n",
      "Epoch 43, Batch 500, Loss 3.6295907497406006\n",
      "Epoch 43, Batch 600, Loss 3.6914026737213135\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 43 completed with average loss 3.6884 in 145.90s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 44, Batch 0, Loss 3.663450002670288\n",
      "Epoch 44, Batch 100, Loss 3.5527820587158203\n",
      "Epoch 44, Batch 200, Loss 3.6199700832366943\n",
      "Epoch 44, Batch 300, Loss 3.6153955459594727\n",
      "Epoch 44, Batch 400, Loss 3.728891372680664\n",
      "Epoch 44, Batch 500, Loss 3.629288673400879\n",
      "Epoch 44, Batch 600, Loss 3.7087035179138184\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 44 completed with average loss 3.6723 in 145.98s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 45, Batch 0, Loss 3.5272598266601562\n",
      "Epoch 45, Batch 100, Loss 3.774477958679199\n",
      "Epoch 45, Batch 200, Loss 3.6202151775360107\n",
      "Epoch 45, Batch 300, Loss 3.612304925918579\n",
      "Epoch 45, Batch 400, Loss 3.517934560775757\n",
      "Epoch 45, Batch 500, Loss 3.7948484420776367\n",
      "Epoch 45, Batch 600, Loss 3.732588291168213\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 45 completed with average loss 3.6546 in 145.82s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 46, Batch 0, Loss 3.509289503097534\n",
      "Epoch 46, Batch 100, Loss 3.5685057640075684\n",
      "Epoch 46, Batch 200, Loss 3.7534470558166504\n",
      "Epoch 46, Batch 300, Loss 3.69514799118042\n",
      "Epoch 46, Batch 400, Loss 3.622175931930542\n",
      "Epoch 46, Batch 500, Loss 3.6608145236968994\n",
      "Epoch 46, Batch 600, Loss 3.6725058555603027\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 46 completed with average loss 3.6366 in 145.90s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 47, Batch 0, Loss 3.5612149238586426\n",
      "Epoch 47, Batch 100, Loss 3.5980303287506104\n",
      "Epoch 47, Batch 200, Loss 3.6209042072296143\n",
      "Epoch 47, Batch 300, Loss 3.6099514961242676\n",
      "Epoch 47, Batch 400, Loss 3.670436143875122\n",
      "Epoch 47, Batch 500, Loss 3.6247012615203857\n",
      "Epoch 47, Batch 600, Loss 3.6617047786712646\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 47 completed with average loss 3.6206 in 145.95s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 48, Batch 0, Loss 3.4134998321533203\n",
      "Epoch 48, Batch 100, Loss 3.5859005451202393\n",
      "Epoch 48, Batch 200, Loss 3.4662365913391113\n",
      "Epoch 48, Batch 300, Loss 3.604931354522705\n",
      "Epoch 48, Batch 400, Loss 3.652604103088379\n",
      "Epoch 48, Batch 500, Loss 3.6006979942321777\n",
      "Epoch 48, Batch 600, Loss 3.715040683746338\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 48 completed with average loss 3.6048 in 145.87s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 49, Batch 0, Loss 3.5043280124664307\n",
      "Epoch 49, Batch 100, Loss 3.5925850868225098\n",
      "Epoch 49, Batch 200, Loss 3.506121873855591\n",
      "Epoch 49, Batch 300, Loss 3.700427770614624\n",
      "Epoch 49, Batch 400, Loss 3.676945686340332\n",
      "Epoch 49, Batch 500, Loss 3.664351463317871\n",
      "Epoch 49, Batch 600, Loss 3.5991640090942383\n",
      "\n",
      "------------------------------------------------------------\n",
      "Epoch 49 completed with average loss 3.5898 in 145.79s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Total training time: 2h 1m 31s\n"
     ]
    }
   ],
   "source": [
    "# Call the train function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "train(model, epochs, optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8667411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, num_words, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the starting text and initialize the hidden state\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # Generate words one by one\n",
    "    for _ in range(num_words):\n",
    "        # Input tensor preparation\n",
    "        input_indices = [vocab[word] for word in words[-1:]]  # get indices for the last word only\n",
    "        x = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "        \n",
    "        # Get predictions from model\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        \n",
    "        # Vector of raw prediction score (logit), apply softmax with temperature\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        probabilities = F.softmax(last_word_logits / temperature, dim=0).detach().cpu().numpy()\n",
    "        \n",
    "        # Sample a word index from the probability distribution, append the generated word to the words list\n",
    "        word_index = np.random.choice(len(vocab), p=probabilities)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2884543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like the apollo , <unk> <unk> en <unk> from <unk> to 1611 . four distinguished his worship , there is not what the quality of god seems to have written the <unk> , israeli <unk> and other restoration inscriptions . some of these acts does not give sarnia songs with the accompanying name of the french island and <unk> from medieval kings . this contains small examples of collectively used as georgian <unk> , and honoring elijah <unk> , 27 trials , whatever malalas ' prayers , , for the <unk> front ) , or <unk> ( series alongside microphones )\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"I like\", num_words=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78eabe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my cat is black @-@ like <unk> . unfortunately , for instance , provided @-@ old white and purple juice <unk> . many of the victims states they be worth eaten as approximately 23 <unk> that is of 8 % . the children to <unk> and four degree receive stories who described the <unk> in myth , so they found much good actively of privacy and text in the very fashion goal , though he ' s ill ' like ' it wasn ' t . in the meantime , the production of its worldwide was ever not written . , for\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"A cat\", num_words=125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb126a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she loves you because she wanted a new musical to <unk> , moving from it to a varied <unk> <unk> on the words yes being was <unk> . lady rosebery of an audience was able to admit that it was on right . the age of i did understand the freedom of it where he could not gain public topics and do not say the <unk> , except it simply it . his political involvement in the purge was an effort of news . however , relations with the country would not be granted until the incursion was discussed by louis @-@ foot (\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"She loves you because\", num_words=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6de373d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hope . turner explained , i think i hadn ' t adopted the cumulative video of the day when neither <unk> can be given and forever is love and his want , as is the song ' s ten  american title . n . <unk> claimed that my life is good with grace , but the case was destined to die . it ' s enough to perform you really want something to have been adventure , while <unk> dead plays , i ' m really making you ' re bit too , without everything i need to make you\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"I hope\", num_words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be96ae3-0322-4e33-a9c9-d4d610b8a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the beatles <unk> ( <unk> ) , drama ) dublin and themes of fame , and bungie ( 1994 ) , as their best @-@ lived albums of 2003 , all things called samples  1992 . the songs chose to dominate the song in woodstock and country , which follows its background on the show ' s most different part of the show , but that it makes more erratic , too really in what me not ' her ' , i go as a <unk> deal , but in negative desert he couldn ' t play on our side to\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"The Beatles are\", num_words=250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c86240a2-b616-41ee-9ed3-89fbbb7f0387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super mario is the oldest time to have altered a historical tradition or how it she is almost physically . it was only used as a threat place of the latin american empire when bertin successfully become emperor of his followers , leading to several archaeologists , who would outside the term wica as the mission to gifted high native , but it conveyed word insight over the growing importance of the anonymus period . while the corrector became <unk> of the sinai period , the victory included the river between ancient britain and <unk>  <unk> ad . later that year , murchad discovered buried shapur on assyria or mohammed . eusebius is dismissed as the ruler of sidon , and a <unk> of his wife would have been allowed as a beautiful ruler of the patriarch ' s son of the arabs of kingdom p . lawrence ' s <unk> <unk> ( sitriuc mac mal ) during the 17th century , and <unk> declared gofraid as the <unk> of the kings of king ' s trilogy\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_text=\"Super Mario is\", num_words=175))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2bdc3-ad0d-426d-8349-4327f9511239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
